---
title: "Data Import and Preparation"
author: "Eugene CHEONG Wei Herng"
date: "10/22/2020"
output:
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_float: true
    toc_depth: 4
    code:folding: show
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Import Required Libraries

We use the following code chunk to install and load the following libraries:

- tidyverse: for data cleaning and wrangling.
- httr: to make HTTP GET requests to the OneMap Search API in order to collect the geometry for each postal code.
- jsonlite: to manipulate the response of the HTTP GET requests, which is returned in JSON format.
- readxl: to read excel files.

```{r}
packages = c('tidyverse', 'httr', 'jsonlite', 'readxl')
for (p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
    }
  library(p,character.only = T)
}
```

# Import and Prepare Aspatial Data

The code chunk below defines *file_dir* and *file_list*, which will be passed in as function parameters for the later code chunks.

```{r}
file_dir <- "data/aspatial/"
file_list <- c("acra-information-on-corporate-entities-a.csv",
               "acra-information-on-corporate-entities-b.csv",
               "acra-information-on-corporate-entities-c.csv",
               "acra-information-on-corporate-entities-d.csv",
               "acra-information-on-corporate-entities-e.csv",
               "acra-information-on-corporate-entities-f.csv",
               "acra-information-on-corporate-entities-g.csv",
               "acra-information-on-corporate-entities-h.csv",
               "acra-information-on-corporate-entities-i.csv",
               "acra-information-on-corporate-entities-j.csv",
               "acra-information-on-corporate-entities-k.csv",
               "acra-information-on-corporate-entities-l.csv",
               "acra-information-on-corporate-entities-m.csv",
               "acra-information-on-corporate-entities-n.csv",
               "acra-information-on-corporate-entities-o.csv",
               "acra-information-on-corporate-entities-others.csv",
               "acra-information-on-corporate-entities-p.csv",
               "acra-information-on-corporate-entities-q.csv",
               "acra-information-on-corporate-entities-r.csv",
               "acra-information-on-corporate-entities-s.csv",
               "acra-information-on-corporate-entities-t.csv",
               "acra-information-on-corporate-entities-u.csv",
               "acra-information-on-corporate-entities-v.csv",
               "acra-information-on-corporate-entities-w.csv",
               "acra-information-on-corporate-entities-x.csv",
               "acra-information-on-corporate-entities-y.csv",
               "acra-information-on-corporate-entities-z.csv")
```

The following code chunk defines a function, *prep_data()*, which:

1. Goes into the directory stated in *file_dir*,
1. Loops through the files in the *file_list*,
1. Filters the data to only include companies registered between *start_year* and *end_year*,
1. Filters for Live Companies,
1. Create or append to a dataframe known as ***results_csv***.
1. After looping through every file in the *file_list*, it will write the resulting dataframe into a .csv file called ***sg_corp_info.csv***.

As it successfully processes each file or when the function has been succecssfully executed, it will print the corresponding message.

```{r}
prep_data <- function(file_dir, file_list, start_year, end_year) {
  for (file in file_list) {
    biz_csv <- read_csv(paste0(file_dir, file),
                        col_types = cols(primary_ssic_code = "c")) %>%
      filter(entity_status_description %in% c("Live", "Live Company")) %>%
      mutate(registration_year = format(as.Date(uen_issue_date, format="%Y-%m-%d"),"%Y")) %>%
      filter(registration_year >= start_year & registration_year <= end_year) %>%
      mutate(`primary_ssic_code` = substr(primary_ssic_code, start = 1, stop = 2)) %>%
      select("uen",
         "registration_year",
         "primary_ssic_code",
         "postal_code")
    if (!exists("result_csv")) {
      result_csv <- data.frame(biz_csv)
    } else {
      result_csv <- bind_rows(result_csv, biz_csv)
      print(paste0(file, " has been successfully processed."))
    }
  }
  write_csv(result_csv, "data/aspatial/sg-corp-info.csv")
  print("Completed. Please see newly created .csv file for results.")
}
```

The following code chunk calls the *prep_data()* function defined in the code chunk above so that only Live Companies registered between the years 2017 and 2019 will remain, as these are the entities that are relevant to the project.

```{r}
prep_data(file_dir, file_list, 2017, 2019)
```

# Collect Data from OneMap API

## Testing Requests using Sample Postal Code

The following code chunk is used to derive the API call destination for a sample API call, to retrieve the geometry for the address code 579722.

```{r}
corp_info <- read_csv("data/aspatial/sg-corp-info.csv")
baseURL <- "https://developers.onemap.sg/"
end_point <- "commonapi/search?"
param_search <- "searchVal="
other_param <- "&returnGeom=Y&getAddrDetails=N"
postal_code = "579722"
API_call <- paste0(baseURL, end_point, param_search, postal_code, other_param)
API_call
```

The following code chunk executes the API call using a HTTP GET request.

```{r}
req <- GET(API_call)
```

We then convert the JSON response using the code chunk below.

```{r}
json <- content(req, as = "text")
json_info <- fromJSON(json)
```

We obtain the X-coordinates for the sample postal code using the code chunk below.

```{r}
json_info$results["X"]
```

## Defining Actual Function

Now that we have tested out how to make an API call using a single postal code, let us define a function known as *call_onemap()* to "automate" the process for the many postal codes we have. This function takes in a parameter, the postal code, and returns a vector containing the X and Y-coordinates for that postal code respectively.

```{r}
call_onemap <- function(postal_code) {
  corp_info <- read_csv("data/aspatial/sg-corp-info.csv")
  baseURL <- "https://developers.onemap.sg/"
  end_point <- "commonapi/search?"
  param_search <- "searchVal="
  other_param <- "&returnGeom=Y&getAddrDetails=N"
  req <- GET(paste0(baseURL, end_point, param_search, postal_code, other_param))
  json <- content(req, as = "text")
  json_info <- fromJSON(json)
  return (c(json_info$results["X"], json_info$results["Y"]))
}
```

However, before calling the function, we want to avoid overloading OneMap's servers by reducing the number of unnecessary API calls. To do this, we import the just-created *sg-corp-info.csv* file, and find the unique postal codes. We than cast it as a data frame, and renamed the columns. We then create 2 new columns to hold the co-ordinates once we obtained it from the API. This is done in the code chunk below.

```{r}
corp_info <- read_csv("data/aspatial/sg-corp-info.csv")

length(unique(corp_info$postal_code))

unique_postal_code <- as.data.frame(unique(corp_info$postal_code))
colnames(unique_postal_code)[colnames(unique_postal_code) == "unique(corp_info$postal_code)"] <- "postal_code"

postal_code_geom <- unique_postal_code %>%
  mutate(X_coord = as.numeric(1.1), Y_coord = as.numeric(1.1))
```

Now, we can execute the function *call_onemap()* which we have defined earlier. The code chunk below converts every unique postal code value and calls the OneMap Search API and assigns the resulting coordinates into the respective fields. If no geometry was returned, NA is assigned instead. This code chunk also uses Sys.sleep() to impose rate limits so that the OneMap server will not experience a heavy server load.

We decided to split the work into batches using the for loop, where we ran it multiple times, using a few thousand rows which is declared in line 189 instead of running everything once.

```{r warning=FALSE, message=FALSE}
for (row_num in 20001:24217) {
  postal_code <- postal_code_geom[row_num,]["postal_code"]
  if (nchar(postal_code) == 5) {
    postal_code <- paste0("0", postal_code)
  }
  if (!(postal_code_geom[row_num,]["X_coord"] == as.numeric(1.1))) next
  else {
    Sys.sleep(0.8)
    tryCatch({coords <- call_onemap(postal_code)},
             error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
    if ("X" %in% names(coords)) {
      postal_code_geom[row_num, ]["X_coord"] <- as.numeric(coords[["X"]][1])
      postal_code_geom[row_num, ]["Y_coord"] <- as.numeric(coords[["Y"]][1])     
    }
    else {
      postal_code_geom[row_num, ]["X_coord"] <- NA
      postal_code_geom[row_num, ]["Y_coord"] <- NA
    }

  }
}
```

The result is then written to a .csv file named *postal_code_geom.csv*. If there are still rows not called using the function call above, the code chunk above is re-ran using a new batch of row numbers which is specified in the row num, at line 189.

```{r}
write_csv(postal_code_geom, "data/aspatial/postal_code_geom.csv")
```

# Clean SSIC file, title based primary ssic code, remove columns that are redundant

We use the following code chunk to prepare the SSIC Classification Information so that we can combine it with our other data files.

```{r}
ssic <- read_excel('data/aspatial/ssic2020-classification-structure.xlsx')
ssic <- ssic[-c(1:3),]
names(ssic) <- ssic[1,]
ssic <- ssic[-c(1),]
ssic <- ssic %>%
  filter((nchar(`SSIC 2020`) == 2))

ssic$`category` <- "A"

ssic$`category`[ssic$`SSIC 2020` %in% c('1','2','3')] <- "A"
ssic$`category`[ssic$`SSIC 2020` %in% c('8','9')] <- "B"
ssic$`category`[ssic$`SSIC 2020` %in% c('10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32')] <- "C"
ssic$`category`[ssic$`SSIC 2020` %in% c('35')] <- "D"
ssic$`category`[ssic$`SSIC 2020` %in% c('36','37','38')] <- "E"
ssic$`category`[ssic$`SSIC 2020` %in% c('41','42','43')] <- "F"
ssic$`category`[ssic$`SSIC 2020` %in% c('46','47')] <- "G"
ssic$`category`[ssic$`SSIC 2020` %in% c('49','50','51','52','53')] <- "H"
ssic$`category`[ssic$`SSIC 2020` %in% c('55','56')] <- "I"
ssic$`category`[ssic$`SSIC 2020` %in% c('58','59','60','61','62','63')] <- "J"
ssic$`category`[ssic$`SSIC 2020` %in% c('64','65','66')] <- "K"
ssic$`category`[ssic$`SSIC 2020` %in% c('68')] <- "L"
ssic$`category`[ssic$`SSIC 2020` %in% c('69','70','71','72','73','74','75')] <- "M"
ssic$`category`[ssic$`SSIC 2020` %in% c('77','78','79','80','81','82')] <- "N"
ssic$`category`[ssic$`SSIC 2020` %in% c('84')] <- "O"
ssic$`category`[ssic$`SSIC 2020` %in% c('85')] <- "P"
ssic$`category`[ssic$`SSIC 2020` %in% c('86','87','88')] <- "Q"
ssic$`category`[ssic$`SSIC 2020` %in% c('90','91','92','93')] <- "R"
ssic$`category`[ssic$`SSIC 2020` %in% c('94','95','96')] <- "S"
ssic$`category`[ssic$`SSIC 2020` %in% c('97')] <- "T"
ssic$`category`[ssic$`SSIC 2020` %in% c('99')] <- "U"
```

## Clean SSIC file, title based A to U

```{r}
ssic1 <- read_excel('data/aspatial/ssic2020-classification-structure.xlsx')
ssic1 <- ssic1[-c(1:3),]
names(ssic1) <- ssic1[1,]
ssic1 <- ssic1[-c(1),]
ssic1 <- ssic1 %>%
  filter((nchar(`SSIC 2020`) == 1))

```

## Join both ssic dataframe, title based on category

```{r}
ssic <- left_join(ssic,ssic1,by=c('category' = "SSIC 2020")) %>%
  select(-`SSIC 2020 Title.x`)
```

```{r}
names(ssic)[names(ssic) == 'SSIC 2020 Title.y'] <- 'primary_ssic_code'
write_csv(ssic, "data/aspatial/ssic2020.csv")
```


# Join corp_info with ssic categories and respective descriptions

We use the following code chunk to join our Corporate Entity Information with the SSIC Classification Code based on their SSIC Code.

```{r}
corp_info <- left_join(corp_info,ssic2020,by=c('primary_ssic_code' = "SSIC 2020"))
names(corp_info)[names(corp_info) == 'primary_ssic_code.y'] <- 'primary_ssic_category_description'
```

We use the following code chunk to check for any NA in our corp_info and postal_code_geom.

```{r}
sum(is.na(corp_info))
sum(is.na(postal_code_geom))
```

We use the following code chunk to join our corporate entity information with the postal code geometry based on their postal code.

```{r}
postal_code_geom$postal_code <- as.character(postal_code_geom$postal_code)
corp_info <- left_join(corp_info,postal_code_geom,by=c('postal_code' = 'postal_code'))
```

We then check if there are any NA values in the result.

```{r}
sum(is.na(corp_info))
```

Since there are NA values, which indicates that their co-ordinates could not be found using the OneMap API, we decided to drop these rows.

```{r}
corp_info <- na.omit(corp_info)
```

We then write the result to our *corp_info_merged.csv* file which will acts as a starting point for our spatial point patterns analysis and our geographic segmentation.

```{r}
write_csv(corp_info, "data/aspatial/corp_info_merged.csv")

```

